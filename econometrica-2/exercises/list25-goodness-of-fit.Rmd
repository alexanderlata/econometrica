---
title: "Задачи по Эконометрике-2: Качество подгонки и сравнение моделей"
author: "Н.В. Артамонов (МГИМО МИД России)"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
    number_sections: true
    df_print: paged
  word_document:
    toc: true
  pdf_document:
    toc: true
    number_sections: true
    df_print: default
    latex_engine: xelatex
header-includes:
- \usepackage{xltxtra}
- \usepackage{fontspec}
- \setmainfont{Times New Roman}
- \setsansfont{Arial}
- \setmonofont{Courier New}
- \newfontfamily{\cyrillicfont}{Times New Roman}
- \newfontfamily{\cyrillicfonttt}{Courier New}
- \newfontfamily{\cyrillicfontsf}{Arial}
- \usepackage{xcolor}
- \usepackage[english,russian]{babel}
---

```{r, message=FALSE, echo=FALSE}
library(stargazer)
library(lmtest)
library(car)
library(sandwich)
library(jtools)
library(DescTools)
library(performance)
data(loanapp, package = 'wooldridge')
data(SwissLabor, package = 'AER')
mroz_Greene <- read.csv('https://raw.githubusercontent.com/artamonoff/econometrica/refs/heads/main/econometrica-2/datasets/TableF5-1.csv')
```

# Качество подгонки

## labour force equation #1 (probit)

Для датасета `TableF5-1.csv` рассмотрим несколько probit-регрессий. Результаты оценивания

```{r, echo=FALSE, message=FALSE, warning=FALSE, comment=''}
my.digits <- 3
my.digits.output <- 4
sign.level <- 0.10
regr <- NULL
regr[[1]] <- glm(formula=LFP~WA+I(WA^2)+WE+KL6+K618+CIT+UN+log(FAMINC), data=mroz_Greene,
                 family = binomial(link='probit') )
regr[[2]] <- update(regr[[1]], formula. = ~.-WA-I(WA^2) )
regr[[3]] <- update(regr[[1]], formula. = ~.-CIT-UN-K618 )
regr[[4]] <- update(regr[[1]], formula. = ~.-CIT-UN-K618-log(FAMINC) )
stargazer(regr, type='text', digits=my.digits.output, digit.separator = '', 
          dep.var.caption = 'Зависимая переменная', df=FALSE)

pseudo.R2 <- NULL
adj.pseudo.R2 <-NULL
CoxSnell <- NULL
Nagelkerke <- NULL
Efron <- NULL
McKelvey <- NULL
Zavoina <- NULL

for(i in 1:length(regr)) {
  pseudo.R2 <- c(pseudo.R2, PseudoR2(regr[[i]], which='McFadden') )
  adj.pseudo.R2 <- c(adj.pseudo.R2, 1-(-regr[[i]]$deviance/2-regr[[i]]$rank+1)/(-regr[[i]]$null.deviance/2) )
  CoxSnell <- c(CoxSnell, PseudoR2(regr[[i]], which='CoxSnell') )
  Nagelkerke <- c(Nagelkerke, PseudoR2(regr[[i]], which='Nagelkerke') )
  Efron <- c(Efron, PseudoR2(regr[[i]], which='Efron') )
  McKelvey <- c(McKelvey, PseudoR2(regr[[i]], which='McKelveyZavoina') )
  # pseudo.R2 <- c(pseudo.R2, as.numeric(r2_mcfadden(regr[[i]])[[1]]) )
  # adj.pseudo.R2 <- c(adj.pseudo.R2, as.numeric(r2_mcfadden(regr[[i]])[[2]]))
  # CoxSnell <- c(CoxSnell, r2_coxsnell(regr[[i]]) )
  # Nagelkerke <- c(Nagelkerke, r2_nagelkerke(regr[[i]]) )
  # Efron <- c(Efron, r2_efron(regr[[i]]))
  # McKelvey <- c(McKelvey, r2_mckelvey(regr[[i]]))
}
```

Для каждой регрессии вычислите следующие показатели качества подгонки модели: \(R^2_{pseudo}\), \(adj.R^2_{pseudo}\), 
Cox & Snell, Nagelkerke/Cragg & Uhler, Efron, McKelvey & Zavoina. **Ответ округлите до `r my.digits`-х десятичных знаков.**

Ответ: 

```{r, echo=FALSE, comment=''}
goodness.of.fit <- data.frame(Model=1:length(regr),
                              pseudo.R2=pseudo.R2,
                              adj.pseudo.R2=adj.pseudo.R2,
                              CoxSnell=CoxSnell,
                              Nagelkerke=Nagelkerke,
                              Efron = Efron,
                              McKelveyZavoina = McKelvey
                              )
stargazer(goodness.of.fit, type='text', summary=FALSE, rownames=FALSE, digits = my.digits)
```

## approve equation #1 (logit)

Для датасета `loanapp` рассмотрим несколько logit-регрессий. Результаты оценивания

```{r, echo=FALSE, message=FALSE, warning=FALSE, comment=''}
my.digits <- 3
my.digits.output <- 4
sign.level <- 0.01
regr <- NULL
regr[[1]] <- glm(formula=approve~appinc+I(appinc^2)+mortno+unem+dep+male+married+yjob+self, data=loanapp,
                 family = binomial(link='logit') )
regr[[2]] <- update(regr[[1]], formula. = ~.-appinc-I(appinc^2) )
regr[[3]] <- update(regr[[1]], formula. = ~.-male-yjob-self )
regr[[4]] <- update(regr[[1]], formula. = ~.-male-yjob-self-unem-married )
stargazer(regr, type='text', digits=my.digits.output, digit.separator = '', 
          dep.var.caption = 'Зависимая переменная', df=FALSE)

pseudo.R2 <- NULL
adj.pseudo.R2 <-NULL
CoxSnell <- NULL
Nagelkerke <- NULL
Efron <- NULL
McKelvey <- NULL

for(i in 1:length(regr)) {
  pseudo.R2 <- c(pseudo.R2, PseudoR2(regr[[i]], which='McFadden') )
  adj.pseudo.R2 <- c(adj.pseudo.R2, 1-(-regr[[i]]$deviance/2-regr[[i]]$rank+1)/(-regr[[i]]$null.deviance/2) )
  CoxSnell <- c(CoxSnell, PseudoR2(regr[[i]], which='CoxSnell') )
  Nagelkerke <- c(Nagelkerke, PseudoR2(regr[[i]], which='Nagelkerke') )
  Efron <- c(Efron, PseudoR2(regr[[i]], which='Efron') )
  McKelvey <- c(McKelvey, PseudoR2(regr[[i]], which='McKelveyZavoina') )
  # pseudo.R2 <- c(pseudo.R2, as.numeric(r2_mcfadden(regr[[i]])[[1]]) )
  # adj.pseudo.R2 <- c(adj.pseudo.R2, as.numeric(r2_mcfadden(regr[[i]])[[2]]))
  # CoxSnell <- c(CoxSnell, r2_coxsnell(regr[[i]]) )
  # Nagelkerke <- c(Nagelkerke, r2_nagelkerke(regr[[i]]) )
  # Efron <- c(Efron, r2_efron(regr[[i]]))
  # McKelvey <- c(McKelvey, r2_mckelvey(regr[[i]]))
}
```

Для каждой регрессии вычислите следующие показатели качества подгонки модели: \(R^2_{pseudo}\), \(adj.R^2_{pseudo}\), 
Cox & Snell, Nagelkerke/Cragg & Uhler, Efron, McKelvey & Zavoina. **Ответ округлите до `r my.digits`-х десятичных знаков.**

Ответ: 

```{r, echo=FALSE, comment=''}
goodness.of.fit <- data.frame(Model=1:length(regr),
                              pseudo.R2=pseudo.R2,
                              adj.pseudo.R2=adj.pseudo.R2,
                              CoxSnell=CoxSnell,
                              Nagelkerke=Nagelkerke,
                              Efron = Efron,
                              McKelveyZavoina = McKelvey
                              )
stargazer(goodness.of.fit, type='text', summary=FALSE, rownames=FALSE, digits = my.digits)
```

# Сравнение моделей

## labour force equation #1 (probit)

Для датасета `TableF5-1.csv` рассмотрим несколько probit-регрессий. Результаты оценивания

```{r, echo=FALSE, message=FALSE, warning=FALSE, comment=''}
my.digits <- 3
my.digits.output <- 4
sign.level <- 0.10
regr <- NULL
regr[[1]] <- glm(formula=LFP~WA+I(WA^2)+WE+KL6+K618+CIT+UN+log(FAMINC), data=mroz_Greene,
                 family = binomial(link='probit') )
regr[[2]] <- update(regr[[1]], formula. = ~.-WA-I(WA^2) )
regr[[3]] <- update(regr[[1]], formula. = ~.-CIT-UN-K618 )
regr[[4]] <- update(regr[[1]], formula. = ~.-CIT-UN-K618-log(FAMINC) )

for(i in 1:length(regr) ) {
  # regr[[i]]$AIC <- AIC(regr[[i]])
  regr[[i]]$BIC <- BIC(regr[[i]])
}

my.logL.null <- -regr[[1]]$null.deviance/2
n <- nobs(regr[[1]])

stargazer(regr, type='text', digits=my.digits.output, digit.separator = '', keep.stat = c('n', 'll'),
          dep.var.caption = 'Зависимая переменная', df=FALSE)
```

Для каждой модели вычислите показатели информационных критериев AIC & BIC и \(adj.R^2_{pseudo}\). **Ответ округлите до `r my.digits`-х десятичных знаков.**

Ответ

```{r, echo=FALSE, comment=''}
my.AIC <- NULL
my.BIC <- NULL
my.logL <- NULL
my.k <- NULL
for(i in 1:length(regr)) {
  my.AIC <- c(my.AIC, regr[[i]]$aic)
  my.BIC <- c(my.BIC, regr[[i]]$BIC)
  my.logL <- c(my.logL, -deviance(regr[[i]])/2 )
  my.k <- c(my.k, regr[[i]]$rank-1)
}

info.criterion <- data.frame(Модель=1:length(regr), AIC=my.AIC, BIC=my.BIC, adj.pseudo.R2=1-(my.logL-my.k)/my.logL.null)

stargazer(info.criterion, type='text', summary = FALSE, rownames = FALSE, digit.separator = '', digits=my.digits)
```

Какая модель предпочтительней по информационных критериям и \(adj.R^2_{pseudo}\)?

Ответ: 

```{r, echo=FALSE, comment=''}
df <- data.frame(Критерий=c('AIC', 'BIC', 'adj.pseudo.R2'),
                      Регрессия=c(which.min(my.AIC), which.min(my.BIC), which.max(info.criterion$adj.pseudo.R2) ) )
stargazer(df, type='text', summary = FALSE, rownames = FALSE)
```

## approve equation #1 (logit)

Для датасета `loanapp` рассмотрим несколько probit-регрессий. Результаты оценивания

```{r, echo=FALSE, message=FALSE, warning=FALSE, comment=''}
my.digits <- 3
my.digits.output <- 4
sign.level <- 0.10
regr <- NULL
regr[[1]] <- glm(formula=approve~appinc+I(appinc^2)+mortno+unem+dep+male+married+yjob+self, data=loanapp,
                 family = binomial(link='probit') )
regr[[2]] <- update(regr[[1]], formula. = ~.-appinc-I(appinc^2) )
regr[[3]] <- update(regr[[1]], formula. = ~.-male-yjob-self )
regr[[4]] <- update(regr[[1]], formula. = ~.-male-yjob-self-unem-married )

for(i in 1:length(regr) ) {
  # regr[[i]]$AIC <- AIC(regr[[i]])
  regr[[i]]$BIC <- BIC(regr[[i]])
}

my.logL.null <- -regr[[1]]$null.deviance/2
n <- nobs(regr[[1]])

stargazer(regr, type='text', digits=my.digits.output, digit.separator = '', keep.stat = c('n', 'll'),
          dep.var.caption = 'Зависимая переменная', df=FALSE)
```

Для каждой модели вычислите показатели информационных критериев AIC & BIC и \(adj.R^2_{pseudo}\). **Ответ округлите до `r my.digits`-х десятичных знаков.**

Ответ

```{r, echo=FALSE, comment=''}
my.AIC <- NULL
my.BIC <- NULL
my.logL <- NULL
my.k <- NULL
for(i in 1:length(regr)) {
  my.AIC <- c(my.AIC, regr[[i]]$aic)
  my.BIC <- c(my.BIC, regr[[i]]$BIC)
  my.logL <- c(my.logL, -deviance(regr[[i]])/2 )
  my.k <- c(my.k, regr[[i]]$rank-1)
}

info.criterion <- data.frame(Модель=1:length(regr), AIC=my.AIC, BIC=my.BIC, adj.pseudo.R2=1-(my.logL-my.k)/my.logL.null)

stargazer(info.criterion, type='text', summary = FALSE, rownames = FALSE, digit.separator = '', digits=my.digits)
```

Какая модель предпочтительней по информационных критериям и \(adj.R^2_{pseudo}\)?

Ответ: 

```{r, echo=FALSE, comment=''}
df <- data.frame(Критерий=c('AIC', 'BIC', 'adj.pseudo.R2'),
                      Регрессия=c(which.min(my.AIC), which.min(my.BIC), which.max(info.criterion$adj.pseudo.R2) ) )
stargazer(df, type='text', summary = FALSE, rownames = FALSE)
```